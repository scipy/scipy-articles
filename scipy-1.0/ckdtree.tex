
The cKDTree module was rewritten in C++ with templated classes, and support for
periodic boundary conditions was added. A periodic boundary condition is typically 
used in computer simulations of physical processes.

% TODO: please check / improve literature citations used here
% what about the time complexity of the kNN implementation?
In 2015, we enhanced \texttt{cKDTree.query} with a $k$ nearest neighbors search
parameter. This is an efficient operation\cite{Sproull:1991:RNS:3118219.3118331} 
because it generates and returns a data structure that may be only a fraction 
of the size of the full neighbor list. A C++ implementation is provided that releases
the Python global interpreter lock for the neighbor search process, uses
a memory pool to allocate and automatically reclaim \texttt{structs}, and
heapsort to sort the $k$ nearest neighbors distances, even in the case
of periodic boundary conditions. When $k$ is a non-contiguous list of nearest
neighbors to account for (i.e., \texttt{[1, 3, 4]}), the intervening distances
between the nearest neighbor ($k = 1$) and the farthest neighbor requested
are all interrogated, but only those neighbors that are requested are retained
in memory. Thus, the maximum heap size for this algorithm is conceptually determined
as \texttt{np.arange(max(k))}.

Also in 2015, SciPy added support for generating approximated sparse distance matrices 
between \texttt{KDTree} objects (sets of points organized into a binary space 
partitioning data structure\cite{Bentley:1975:MBS:361002.361007}). 
The user provides a maximum distance value
above which all distances between points in two provided \texttt{KDTree} objects
are set to 0. The distance metric used between k-d trees is not constrained
to the conventional L2 (Euclidean) norm---any Minkowski p-norm value
bounded by 1 and infinity is valid. By default, a dictionary of keys
based sparse matrix is the returned data structure. Previously-published
results had suggested that a hashing approach to sparse matrix assembly
is 7 times faster than constructing with compressed row format (CSR)\cite{10.1007/978-3-540-75755-9_107}.
The dictionary object maps row and column indices to elements of the sparse matrix, which is
efficient for matrix construction because all zero values are skipped in
the dictionary mapping. The C++ level sparse matrix construction releases the Python
global interpreter lock for increased performance. Once constructed, the
dictionary of keys sparse matrix has the amortized constant time complexity 
($O(1)$) for distance value retrieval that one would expect from a 
dictionary\cite{Cormen:2001:IA:580470}. For efficiently performing arithmetic
operations on the sparse matrix, SciPy allows the dictionary of keys
sparse matrix to be directly converted to the common CSR, CSC, and COO
data structures.

The cKDTree module implements a dual tree counting algorithm\cite{Moore2000ar},
with an improvement to the pair counting algorithm to improve the scaling
with the number of bins. The cKDTree can now be augmented by weights, with 
weighted paircount essential in many scientific applications, e.g. computing 
correlation functions of galaxies\cite{0004-637X-750-1-38}.

\textbf{Add a Figure to show the scaling, before and after.}
\textbf{perhaps give an example or some formula.}
(cite / mention faster implementions of paircounting algorithms / treecorr, corrfunc)

The main purpose of the paircounting algorithm in cKDTree is to provide a readily
available tool. The KDTree based algorithm is not necessarily the fastest algorithm
depending on the application. E.g. for low number densities a chaining mesh based algorithm
can be easily many times faster than a tree based algorithm\cite{1991ApJ}.

